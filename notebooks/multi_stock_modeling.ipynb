{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "#         Additional Modeling (Random Forest + Plot, LSTM)       #\n",
        "#########################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# For the LSTM portion\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "def bonus_random_forest_plot(df, target_stock=\"AAPL\", extra_features=False, out_dir=\"../results\"):\n",
        "    \"\"\"\n",
        "    Train & plot a RandomForest regression on (OFI + optional extra features) -> next-minute returns of 'target_stock'.\n",
        "    Saves a scatter-plot of predicted vs actual returns.\n",
        "    \"\"\"\n",
        "    ret_col    = f\"log_ret_{target_stock}\"\n",
        "    ofi_cols   = [c for c in df.columns if c.startswith(\"OFI_PCA_\")]\n",
        "\n",
        "    # SHIFT each OFI col by 1 minute\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "    for c in ofi_cols:\n",
        "        X[c+\"_lag1\"] = df[c].shift(1)\n",
        "\n",
        "    if extra_features:\n",
        "        # Example: add mid_price_{target_stock} as a feature, also lagged\n",
        "        price_col = \"mid_price\"\n",
        "        if price_col in df.columns:\n",
        "            X[f\"{price_col}_lag1\"] = df[price_col].shift(1)\n",
        "\n",
        "    # Build target\n",
        "    y = df[ret_col]\n",
        "\n",
        "    # Drop NaNs\n",
        "    tmp = pd.concat([X, y], axis=1).dropna()\n",
        "    if len(tmp) < 50:\n",
        "        print(f\"[RandomForest] Not enough data after dropna. Skipping.\")\n",
        "        return\n",
        "\n",
        "    X_rf = tmp.drop(columns=[ret_col])\n",
        "    y_rf = tmp[ret_col]\n",
        "\n",
        "    # Train/test split\n",
        "    split_idx = int(len(tmp)*0.8)\n",
        "    X_train, X_test = X_rf.iloc[:split_idx], X_rf.iloc[split_idx:]\n",
        "    y_train, y_test = y_rf.iloc[:split_idx], y_rf.iloc[split_idx:]\n",
        "\n",
        "    # Fit a random forest\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict & Evaluate\n",
        "    y_pred = rf.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"[RandomForest] {target_stock}, extra_features={extra_features}, R2={r2:.4f}\")\n",
        "\n",
        "    print(\"Feature Importances:\")\n",
        "    for col, imp in zip(X_rf.columns, rf.feature_importances_):\n",
        "        print(f\"  {col}: {imp:.4f}\")\n",
        "\n",
        "    # Plot predicted vs actual\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.3, label=\"RF Predictions\")\n",
        "    plt.axhline(0, color=\"black\", lw=1)\n",
        "    plt.axvline(0, color=\"black\", lw=1)\n",
        "    plt.xlabel(\"Actual Returns\")\n",
        "    plt.ylabel(\"Predicted Returns\")\n",
        "    plt.title(f\"RF Pred vs Actual, {target_stock}, R2={r2:.4f}\")\n",
        "    plt.legend()\n",
        "    out_png = os.path.join(out_dir, f\"rf_scatter_{target_stock}_features={extra_features}.png\")\n",
        "    plt.savefig(out_png)\n",
        "    plt.close()\n",
        "    print(f\"Saved RandomForest scatter plot to {out_png}\")\n",
        "\n",
        "\n",
        "#############################\n",
        "# LSTM Example with SIGN-based accuracy\n",
        "#############################\n",
        "\n",
        "# 1) Define a custom sign-accuracy metric:\n",
        "def sign_accuracy_metr(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Returns the fraction of matching signs between y_true and y_pred.\n",
        "    For regression, we treat sign(y) in { -1, 0, +1 } (0 is for exact zero).\n",
        "    \"\"\"\n",
        "    y_true_sign = tf.sign(y_true)  # -1.0, 0.0, +1.0\n",
        "    y_pred_sign = tf.sign(y_pred)\n",
        "    equals = tf.cast(tf.equal(y_true_sign, y_pred_sign), tf.float32)\n",
        "    return tf.reduce_mean(equals)\n",
        "\n",
        "def bonus_lstm_example(df, target_stock=\"AAPL\", out_dir=\"../results\"):\n",
        "    \"\"\"\n",
        "    Minimal LSTM example: we try to predict log_ret_{target_stock}(t) from [OFI_PCA_x(t-1), etc].\n",
        "    We'll do a single-step approach with a rolling window for the LSTM, plus a sign-based accuracy metric.\n",
        "    \"\"\"\n",
        "    ret_col  = f\"log_ret_{target_stock}\"\n",
        "    ofi_cols = [c for c in df.columns if c.startswith(\"OFI_PCA_\")]\n",
        "\n",
        "    TIME_STEPS = 5\n",
        "\n",
        "    # Build features\n",
        "    feature_df = pd.DataFrame(index=df.index)\n",
        "    for c in ofi_cols:\n",
        "        feature_df[c] = df[c].fillna(0)\n",
        "\n",
        "    if \"mid_price\" in df.columns:\n",
        "        feature_df[\"mid_price\"] = df[\"mid_price\"].fillna(method=\"ffill\").fillna(0)\n",
        "\n",
        "    for col in feature_df.columns:\n",
        "        feature_df[col+\"_lag1\"] = feature_df[col].shift(1)\n",
        "\n",
        "    feature_cols = [col+\"_lag1\" for col in ofi_cols]\n",
        "    if \"mid_price_lag1\" in feature_df.columns:\n",
        "        feature_cols.append(\"mid_price_lag1\")\n",
        "\n",
        "    y = df[ret_col].copy()\n",
        "\n",
        "    # Drop NaNs\n",
        "    tmp = pd.concat([feature_df[feature_cols], y], axis=1).dropna()\n",
        "\n",
        "    # 2) Rolling window of length TIME_STEPS\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for i in range(TIME_STEPS, len(tmp)):\n",
        "        X_window = tmp.iloc[i - TIME_STEPS : i][feature_cols].values\n",
        "        y_val = tmp.iloc[i][ret_col]\n",
        "        X_list.append(X_window)\n",
        "        y_list.append(y_val)\n",
        "\n",
        "    X_lstm = np.array(X_list)  # (samples, TIME_STEPS, n_feats)\n",
        "    y_lstm = np.array(y_list)\n",
        "    if len(X_lstm) < 50:\n",
        "        print(\"[LSTM] Not enough data. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 3) Train/test split\n",
        "    split_idx = int(len(X_lstm)*0.8)\n",
        "    X_train, X_test = X_lstm[:split_idx], X_lstm[split_idx:]\n",
        "    y_train, y_test = y_lstm[:split_idx], y_lstm[split_idx:]\n",
        "\n",
        "    # 4) Scale\n",
        "    n_samples, ts, n_feats = X_train.shape\n",
        "    X_train_2d = X_train.reshape(n_samples*ts, n_feats)\n",
        "    X_test_2d  = X_test.reshape(len(X_test)*ts, n_feats)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_2d_scaled = scaler.fit_transform(X_train_2d)\n",
        "    X_test_2d_scaled  = scaler.transform(X_test_2d)\n",
        "\n",
        "    X_train_scaled = X_train_2d_scaled.reshape(n_samples, ts, n_feats)\n",
        "    X_test_scaled  = X_test_2d_scaled.reshape(len(X_test), ts, n_feats)\n",
        "\n",
        "    # 5) Build LSTM\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(16, activation=\"tanh\", input_shape=(ts, n_feats)))\n",
        "    model.add(Dense(1))\n",
        "    # Compile with MSE loss, plus a custom sign_accuracy_metr\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[sign_accuracy_metr])\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_data=(X_test_scaled, y_test),\n",
        "        epochs=10, batch_size=32, verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_scaled).flatten()\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"[LSTM] {target_stock} R2={r2:.4f}\")\n",
        "\n",
        "    # 6) Plot predicted vs. actual\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.3, label=\"LSTM Predictions\")\n",
        "    plt.axhline(0, color=\"black\", lw=1)\n",
        "    plt.axvline(0, color=\"black\", lw=1)\n",
        "    plt.xlabel(\"Actual Returns\")\n",
        "    plt.ylabel(\"Predicted Returns\")\n",
        "    plt.title(f\"LSTM Pred vs Actual, {target_stock}, R2={r2:.4f}\")\n",
        "    plt.legend()\n",
        "    out_png = os.path.join(out_dir, f\"lstm_scatter_{target_stock}.png\")\n",
        "    plt.savefig(out_png)\n",
        "    plt.close()\n",
        "    print(f\"Saved LSTM scatter plot to {out_png}\")\n",
        "\n",
        "    # 7) Plot training curve for LOSS\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(f\"LSTM Training Curve (Loss) {target_stock}\")\n",
        "    plt.legend()\n",
        "    out_png2 = os.path.join(out_dir, f\"lstm_training_loss_{target_stock}.png\")\n",
        "    plt.savefig(out_png2)\n",
        "    plt.close()\n",
        "    print(f\"Saved LSTM training loss curve to {out_png2}\")\n",
        "\n",
        "    # 8) Plot sign-based ACCURACY\n",
        "    # We stored it in history.history[\"sign_accuracy_metr\"] and [\"val_sign_accuracy_metr\"]\n",
        "    train_acc_key = \"sign_accuracy_metr\"\n",
        "    val_acc_key   = \"val_sign_accuracy_metr\"\n",
        "    if train_acc_key in history.history and val_acc_key in history.history:\n",
        "        plt.figure()\n",
        "        plt.plot(history.history[train_acc_key], label=\"train_sign_acc\")\n",
        "        plt.plot(history.history[val_acc_key],   label=\"val_sign_acc\")\n",
        "        plt.title(f\"LSTM Training Curve (Sign Accuracy) {target_stock}\")\n",
        "        plt.legend()\n",
        "        out_png3 = os.path.join(out_dir, f\"lstm_training_acc_{target_stock}.png\")\n",
        "        plt.savefig(out_png3)\n",
        "        plt.close()\n",
        "        print(f\"Saved LSTM training sign-accuracy curve to {out_png3}\")\n",
        "    else:\n",
        "        print(\"Sign accuracy keys not found in history; skipping accuracy plot.\")\n",
        "\n",
        "\n",
        "def run_bonus_models(df_cross, out_dir=\"../results\"):\n",
        "    \"\"\"\n",
        "    Demonstrate calling the RandomForest + LSTM code for a chosen stock (e.g. AAPL).\n",
        "    \"\"\"\n",
        "    # 1) Random Forest with/without extra features\n",
        "    bonus_random_forest_plot(df_cross, target_stock=\"AAPL\", extra_features=False, out_dir=out_dir)\n",
        "    bonus_random_forest_plot(df_cross, target_stock=\"AAPL\", extra_features=True,  out_dir=out_dir)\n",
        "\n",
        "    # 2) LSTM example\n",
        "    bonus_lstm_example(df_cross, target_stock=\"AAPL\", out_dir=out_dir)\n",
        "\n",
        "run_bonus_models(df_cross, out_dir=RESULTS_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz1S3-1Ca-op",
        "outputId": "1967f6a6-e423-4c40-eab8-735c3a5e0df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RandomForest] AAPL, extra_features=False, R2=-0.1635\n",
            "Feature Importances:\n",
            "  OFI_PCA_AAPL_lag1: 0.2634\n",
            "  OFI_PCA_AMGN_lag1: 0.1471\n",
            "  OFI_PCA_TSLA_lag1: 0.1950\n",
            "  OFI_PCA_JPM_lag1: 0.1850\n",
            "  OFI_PCA_XOM_lag1: 0.2095\n",
            "Saved RandomForest scatter plot to ../results/rf_scatter_AAPL_features=False.png\n",
            "[RandomForest] AAPL, extra_features=True, R2=-0.1635\n",
            "Feature Importances:\n",
            "  OFI_PCA_AAPL_lag1: 0.2634\n",
            "  OFI_PCA_AMGN_lag1: 0.1471\n",
            "  OFI_PCA_TSLA_lag1: 0.1950\n",
            "  OFI_PCA_JPM_lag1: 0.1850\n",
            "  OFI_PCA_XOM_lag1: 0.2095\n",
            "Saved RandomForest scatter plot to ../results/rf_scatter_AAPL_features=True.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0225 - sign_accuracy_metr: 0.2787 - val_loss: 0.0073 - val_sign_accuracy_metr: 0.3850\n",
            "Epoch 2/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0014 - sign_accuracy_metr: 0.2800 - val_loss: 0.0034 - val_sign_accuracy_metr: 0.3867\n",
            "Epoch 3/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7.8907e-04 - sign_accuracy_metr: 0.2753 - val_loss: 0.0023 - val_sign_accuracy_metr: 0.3825\n",
            "Epoch 4/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5.5621e-04 - sign_accuracy_metr: 0.2806 - val_loss: 0.0018 - val_sign_accuracy_metr: 0.3841\n",
            "Epoch 5/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.1474e-04 - sign_accuracy_metr: 0.2875 - val_loss: 0.0015 - val_sign_accuracy_metr: 0.3802\n",
            "Epoch 6/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.1611e-04 - sign_accuracy_metr: 0.2836 - val_loss: 0.0014 - val_sign_accuracy_metr: 0.3827\n",
            "Epoch 7/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3381e-04 - sign_accuracy_metr: 0.2824 - val_loss: 0.0012 - val_sign_accuracy_metr: 0.3747\n",
            "Epoch 8/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1018e-04 - sign_accuracy_metr: 0.2788 - val_loss: 0.0010 - val_sign_accuracy_metr: 0.3778\n",
            "Epoch 9/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6585e-04 - sign_accuracy_metr: 0.2853 - val_loss: 9.2514e-04 - val_sign_accuracy_metr: 0.3771\n",
            "Epoch 10/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3304e-04 - sign_accuracy_metr: 0.2801 - val_loss: 8.0212e-04 - val_sign_accuracy_metr: 0.3826\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "[LSTM] AAPL R2=-4427.3081\n",
            "Saved LSTM scatter plot to ../results/lstm_scatter_AAPL.png\n",
            "Saved LSTM training loss curve to ../results/lstm_training_loss_AAPL.png\n",
            "Saved LSTM training sign-accuracy curve to ../results/lstm_training_acc_AAPL.png\n"
          ]
        }
      ]
    }
  ]
}